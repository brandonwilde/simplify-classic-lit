{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentence_scorer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN2FNBFek9rYnfuNie6wULo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5kuQf7ucBDy","executionInfo":{"status":"ok","timestamp":1651797737227,"user_tz":240,"elapsed":18602,"user":{"displayName":"Brandon Wilde","userId":"18127879461253566195"}},"outputId":"b13b0e96-ca4a-40a6-e34a-a17452b7e7db"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqnkYMLjbc81"},"outputs":[],"source":["username = 'feralvam'\n","repository = 'easse'\n","%cd /content/drive/MyDrive/github_repos\n","# !git clone https://github.com/{username}/{repository}\n","%cd {repository}\n","!pip install -e .\n","!pip install pyyaml==5.4.1\n","!pip install sentence_transformers"]},{"cell_type":"code","source":["from collections import Counter\n","from typing import List\n","from tseval.feature_extraction import (\n","    get_compression_ratio,\n","    count_sentence_splits,\n","    get_levenshtein_similarity,\n","    is_exact_match,\n","    get_additions_proportion,\n","    get_deletions_proportion,\n","    get_wordrank_score,\n","    wrap_single_sentence_vectorizer,\n",")\n","import numpy as np\n","from sacrebleu.metrics import BLEU\n","\n","import easse\n","import easse.utils.preprocessing as utils_prep\n","from easse.sari import compute_ngram_stats, compute_macro_sari, compute_micro_sari, compute_precision_recall_f1\n","from easse.bleu import corpus_bleu, corpus_averaged_sentence_bleu\n","from easse.fkgl import FKGLScorer\n","from easse.bertscore import get_bertscore_sentence_scores\n","from bert_score import BERTScorer\n","\n","\n","def sentence_bleu_list(\n","    sys_sents: List[str],\n","    refs_sents: List[List[str]],\n","    smooth_method: str = \"floor\",\n","    smooth_value: float = None,\n","    lowercase: bool = False,\n","    tokenizer: str = \"13a\",\n","    effective_order: bool = True,\n","):\n","    ref_sents = refs_sents[0]\n","    all_bleu = []\n","    for sys_sent, ref_sent in zip(sys_sents, ref_sents):\n","        sys_sent = utils_prep.normalize(sys_sent, lowercase, tokenizer)\n","        ref_sent = utils_prep.normalize(ref_sent, lowercase, tokenizer)\n","\n","        bleu_scorer = BLEU(lowercase=False, force=True, tokenize=\"none\", smooth_method=smooth_method, smooth_value=smooth_value, effective_order=effective_order)\n","        all_bleu.append(round(bleu_scorer.sentence_score(\n","            sys_sent,\n","            [ref_sent],\n","            ).score, 3))\n","\n","    return all_bleu\n","\n","\n","def get_sentence_sari_operation_scores(\n","    orig_sents: List[str],\n","    sys_sents: List[str],\n","    refs_sents: List[List[str]],\n","    lowercase: bool = True,\n","    tokenizer: str = '13a',\n","    legacy=False,\n","    use_f1_for_deletion=True,\n","    use_paper_version=False,\n","):\n","    \"\"\"\n","    Inputs:\n","    orig_sents: list of original sentences (len = n_samples)\n","    sys_sents: list of system sentences (len = n_samples)\n","    refs_sents: list of list of reference sentences (shape = (n_references, n_samples))\n","    legacy: Allows reproducing scores reported in previous work.\n","    It replicates a bug in the original JAVA implementation where only the system outputs and the reference sentences\n","    are further tokenized.\n","    In addition, it assumes that all sentences are already lowercased.\n","    \"\"\"\n","    if legacy:\n","        lowercase = False\n","    else:\n","        orig_sents = [utils_prep.normalize(sent, lowercase, tokenizer) for sent in orig_sents]\n","\n","    sys_sents = [utils_prep.normalize(sent, lowercase, tokenizer) for sent in sys_sents]\n","    refs_sents = [[utils_prep.normalize(sent, lowercase, tokenizer) for sent in ref_sents] for ref_sents in refs_sents]\n","\n","    # -------Edits start----------\n","    all_stats = []\n","    for orig_sent, sys_sent, *ref_sents in zip(orig_sents, sys_sents, *refs_sents):\n","        all_stats.append(compute_ngram_stats([orig_sent], [sys_sent], [ref_sents]))\n","    \n","    all_operations = []\n","    for sent_stats in all_stats:\n","        if not use_paper_version:\n","            add_score, keep_score, del_score = compute_macro_sari(*sent_stats, use_f1_for_deletion=use_f1_for_deletion)\n","            all_operations.append((100*add_score, 100*keep_score, 100*del_score))\n","        else:\n","            add_score, keep_score, del_score = compute_micro_sari(*sent_stats, use_f1_for_deletion=use_f1_for_deletion)\n","            all_operations.append((100*add_score, 100*keep_score, 100*del_score))\n","\n","    return all_operations\n","    # -------Edits end----------\n","\n","def sentence_sari(*args, **kwargs):\n","    \"\"\"Revised corpus_sari function\"\"\"\n","    all_operations = get_sentence_sari_operation_scores(*args, **kwargs)\n","    \n","    all_scores = []\n","    for sent_operations in all_operations:\n","        all_scores.append(round(np.mean(sent_operations), 3))\n","    return all_scores\n","    # return 3\n","\n","\n","def sentence_fkgl(sentences: List[str], tokenizer: str = \"13a\"):\n","\n","    fkgl_scores = []\n","    for sentence in sentences:\n","        scorer = FKGLScorer()\n","        scorer.add(utils_prep.normalize(sentence, tokenizer=tokenizer))\n","        fkgl_scores.append(round(scorer.score(), 3))\n","    \n","    return fkgl_scores\n","\n","\n","\n","def sentence_f1_token(sys_sents: List[str], refs_sents: List[List[str]], lowercase: bool = True, tokenizer: str = '13a'):\n","    def find_correct_tokens(sys_tokens, ref_tokens):\n","        return list((Counter(sys_tokens) & Counter(ref_tokens)).elements())\n","\n","    sys_sents = [utils_prep.normalize(sent, lowercase, tokenizer) for sent in sys_sents]\n","    refs_sents = [[utils_prep.normalize(sent, lowercase, tokenizer) for sent in ref_sents] for ref_sents in refs_sents]\n","\n","    sent_scores = []\n","    f1_token_scores = []\n","    for sys_sent, *ref_sents in zip(sys_sents, *refs_sents):\n","        sys_tokens = sys_sent.split()\n","        sys_total = len(sys_tokens)\n","\n","        candidate_f1_token_scores = []\n","        for ref_sent in ref_sents:\n","            ref_tokens = ref_sent.split()\n","            ref_total = len(ref_tokens)\n","\n","            correct_tokens = len(find_correct_tokens(sys_tokens, ref_tokens))\n","            _, _, f1 = compute_precision_recall_f1(correct_tokens, sys_total, ref_total)\n","            candidate_f1_token_scores.append(f1)\n","\n","        f1_token_scores.append(np.max(candidate_f1_token_scores))\n","        sent_scores.append(round(100.0 * np.mean(f1_token_scores), 3))\n","\n","    return sent_scores\n","\n","\n","def sentence_bertscore(\n","    sys_sents: List[str],\n","    refs_sents: List[List[str]],\n","    lowercase: bool = False,\n","    tokenizer: str = \"13a\",\n","):\n","    all_scores = get_bertscore_sentence_scores(sys_sents, refs_sents, lowercase, tokenizer)\n","    precision, recall, f1 = all_scores\n","\n","    return precision.tolist(), recall.tolist(), f1.tolist()\n","\n","\n","def sentence_cos_similarity(\n","    sys_sents: List[str],\n","    refs_sents: List[List[str]],\n","    lowercase: bool = False,\n","):\n","\n","    from sentence_transformers import SentenceTransformer, util\n","\n","    model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","    def cos_similarity(sent1, sent2):\n","        \"\"\"Calculate cosine similarity of two sentences.\"\"\"\n","        encoded_sents = model.encode([sent1.lower(), sent2.lower()])\n","        cos = util.cos_sim(*encoded_sents)\n","        return round(cos.item(), 3)\n","    \n","    all_sims = []\n","    for sys_sent, ref_sent in zip(sys_sents, refs_sents[0]):\n","        all_sims.append(cos_similarity(sys_sent, ref_sent))\n","\n","    return all_sims\n","\n","\n","\n","def sentence_quality_estimation(\n","    orig_sentences: List[str], sys_sentences: List[str], lowercase: bool = False, tokenizer: str = '13a'):\n","    orig_sentences = [utils_prep.normalize(sent, lowercase, tokenizer) for sent in orig_sentences]\n","    sys_sentences = [utils_prep.normalize(sent, lowercase, tokenizer) for sent in sys_sentences]\n","    \n","    corpus_quality = {\n","    \t'Compression ratio': [],\n","    \t'Sentence splits': [],\n","    \t'Levenshtein similarity': [],\n","    \t'Exact copies': [],\n","    \t'Additions proportion': [],\n","    \t'Deletions proportion': [],\n","    \t'Lexical complexity score': [],\n","    \t}\n","    \n","    for orig_sentence, sys_sentence in zip(orig_sentences, sys_sentences):\n","        corpus_quality['Compression ratio'].append(\n","            round(get_compression_ratio(orig_sentence, sys_sentence), 3))\n","        corpus_quality['Sentence splits'].append(\n","            round(count_sentence_splits(orig_sentence, sys_sentence), 3))\n","        corpus_quality['Levenshtein similarity'].append(\n","            round(get_levenshtein_similarity(orig_sentence, sys_sentence), 3))\n","        corpus_quality['Exact copies'].append(\n","            is_exact_match(orig_sentence, sys_sentence))\n","        corpus_quality['Additions proportion'].append(\n","            round(get_additions_proportion(orig_sentence, sys_sentence), 3))\n","        corpus_quality['Deletions proportion'].append(\n","            round(get_deletions_proportion(orig_sentence, sys_sentence), 3))\n","        corpus_quality['Lexical complexity score'].append(\n","            round(wrap_single_sentence_vectorizer(get_wordrank_score)(orig_sentence, sys_sentence), 3))\n","        \n","    return corpus_quality\n"],"metadata":{"id":"X-RJ86DgcWw-","executionInfo":{"status":"ok","timestamp":1651797807467,"user_tz":240,"elapsed":13008,"user":{"displayName":"Brandon Wilde","userId":"18127879461253566195"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","\n","import click\n","\n","from easse.fkgl import corpus_fkgl\n","from easse.utils.helpers import read_lines\n","from easse.quality_estimation import corpus_quality_estimation\n","from easse.sari import corpus_sari, get_corpus_sari_operation_scores\n","from easse.bleu import corpus_bleu, corpus_averaged_sentence_bleu\n","from easse.compression import corpus_f1_token\n","from easse.utils.constants import (\n","    VALID_TEST_SETS,\n","    VALID_METRICS,\n","    DEFAULT_METRICS,\n",")\n","from easse.utils.resources import get_orig_sents, get_refs_sents\n","from easse.report import write_html_report, write_multiple_systems_html_report\n","\n","\n","def get_sys_sents(test_set, sys_sents_path=None):\n","    # Get system sentences to be evaluated\n","    if sys_sents_path is not None:\n","        return read_lines(sys_sents_path)\n","    else:\n","        # read the system output\n","        with click.get_text_stream(\"stdin\", encoding=\"utf-8\") as system_output_file:\n","            return system_output_file.read().splitlines()\n","\n","def get_orig_and_refs_sents(test_set, orig_sents_path=None, refs_sents_paths=None):\n","    # Get original and reference sentences\n","    if test_set == \"custom\":\n","        assert orig_sents_path is not None\n","        assert refs_sents_paths is not None\n","        if type(refs_sents_paths) == str:\n","            refs_sents_paths = refs_sents_paths.split(\",\")\n","        orig_sents = read_lines(orig_sents_path)\n","        refs_sents = [read_lines(ref_sents_path) for ref_sents_path in refs_sents_paths]\n","    else:\n","        orig_sents = get_orig_sents(test_set)\n","        refs_sents = get_refs_sents(test_set)\n","    # Final checks\n","    assert all(\n","        [len(orig_sents) == len(ref_sents) for ref_sents in refs_sents]\n","    ), f'Not same number of lines for test_set={test_set}, orig_sents_path={orig_sents_path}, refs_sents_paths={refs_sents_paths}'  # noqa: E501\n","    return orig_sents, refs_sents\n","\n","def evaluate_system_output(\n","    test_set,\n","    sys_sents_path=None,\n","    orig_sents_path=None,\n","    refs_sents_paths=None,\n","    tokenizer=\"13a\",\n","    lowercase=True,\n","    metrics=DEFAULT_METRICS,\n","    analysis=False,\n","    quality_estimation=False,\n","):\n","    \"\"\"\n","    Evaluate a system output with automatic metrics.\n","    \"\"\"\n","    VALID_METRICS = [\n","    'bleu',\n","    'sari',\n","    'samsa',\n","    'fkgl',\n","    'sent_bleu',\n","    'f1_token',\n","    'sari_legacy',\n","    'sari_by_operation',\n","    'bertscore',\n","    'cos_sim'\n","]\n","    for metric in metrics:\n","        assert metric in VALID_METRICS, f'\"{metric}\" is not a valid metric. Choose among: {VALID_METRICS}'\n","    sys_sents = get_sys_sents(test_set, sys_sents_path)\n","    orig_sents, refs_sents = get_orig_and_refs_sents(test_set, orig_sents_path, refs_sents_paths)\n","\n","    # compute each metric\n","    metrics_scores = {}\n","    if \"bleu\" in metrics:\n","        metrics_scores[\"bleu\"] = sentence_bleu_list(\n","            sys_sents,\n","            refs_sents,\n","            # force=True,\n","            tokenizer=tokenizer,\n","            lowercase=lowercase,\n","        )\n","\n","    if \"sent_bleu\" in metrics:\n","        metrics_scores[\"sent_bleu\"] = corpus_averaged_sentence_bleu(\n","            sys_sents, refs_sents, tokenizer=tokenizer, lowercase=lowercase\n","        )\n","\n","    if \"sari\" in metrics:\n","        metrics_scores[\"sari\"] = sentence_sari(   # Edited\n","            orig_sents,\n","            sys_sents,\n","            refs_sents,\n","            tokenizer=tokenizer,\n","            lowercase=lowercase,\n","        )\n","\n","    if \"sari_legacy\" in metrics:\n","        metrics_scores[\"sari_legacy\"] = corpus_sari(\n","            orig_sents,\n","            sys_sents,\n","            refs_sents,\n","            tokenizer=tokenizer,\n","            lowercase=lowercase,\n","            legacy=True,\n","        )\n","\n","    if \"sari_by_operation\" in metrics:\n","        (\n","            metrics_scores[\"sari_add\"],\n","            metrics_scores[\"sari_keep\"],\n","            metrics_scores[\"sari_del\"],\n","        ) = get_corpus_sari_operation_scores(\n","            orig_sents,\n","            sys_sents,\n","            refs_sents,\n","            tokenizer=tokenizer,\n","            lowercase=lowercase,\n","        )\n","\n","    if \"samsa\" in metrics:\n","        from easse.samsa import corpus_samsa\n","\n","        metrics_scores[\"samsa\"] = corpus_samsa(\n","            orig_sents,\n","            sys_sents,\n","            tokenizer=tokenizer,\n","            lowercase=lowercase,\n","            verbose=True,\n","        )\n","\n","    if \"fkgl\" in metrics:\n","        metrics_scores[\"fkgl\"] = sentence_fkgl(sys_sents, tokenizer=tokenizer)\n","\n","    if \"f1_token\" in metrics:\n","        metrics_scores[\"f1_token\"] = sentence_f1_token(sys_sents, refs_sents, tokenizer=tokenizer, lowercase=lowercase)\n","\n","    if \"bertscore\" in metrics:\n","        # from easse.bertscore import corpus_bertscore  # Inline import to use EASSE without installing all dependencies\n","\n","        (\n","            metrics_scores[\"bertscore_precision\"],\n","            metrics_scores[\"bertscore_recall\"],\n","            metrics_scores[\"bertscore_f1\"],\n","        ) = sentence_bertscore(sys_sents, refs_sents, tokenizer=tokenizer, lowercase=lowercase)\n","    \n","    if \"cos_sim\" in metrics:\n","        metrics_scores[\"cos_sim\"] = sentence_cos_similarity(sys_sents, refs_sents, lowercase=lowercase)\n","\n","    if analysis:\n","        from easse.annotation.word_level import (\n","            WordOperationAnnotator,\n","        )  # Inline import to use EASSE without installing all dependencies\n","\n","        word_operation_annotator = WordOperationAnnotator(tokenizer=tokenizer, lowercase=lowercase, verbose=True)\n","        metrics_scores[\"word_level_analysis\"] = word_operation_annotator.analyse_operations(\n","            orig_sents, sys_sents, refs_sents, as_str=True\n","        )\n","\n","    if quality_estimation:\n","        metrics_scores[\"quality_estimation\"] = sentence_quality_estimation(\n","            orig_sents, sys_sents, tokenizer=tokenizer, lowercase=lowercase\n","        )\n","\n","    return metrics_scores\n"],"metadata":{"id":"IWLaFq4jo2VG","executionInfo":{"status":"ok","timestamp":1651797821232,"user_tz":240,"elapsed":4416,"user":{"displayName":"Brandon Wilde","userId":"18127879461253566195"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6f3685f7-5cf5-42fa-c173-61be0852a95d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  defaults = yaml.load(f)\n"]}]},{"cell_type":"code","source":["import os\n","data = os.listdir(\"/content/drive/MyDrive/apln552-project/data/raw_data\")\n","data_files = []\n","print(\"Files to process:\")\n","for data_file in data:\n","    if data_file.startswith(\"classic_lit_simplified\"):\n","        print(data_file)\n","        data_files.append(data_file)\n","\n","scores_bleu = []\n","scores_sari = []\n","scores_fkgl = []\n","scores_bertscore_p = []\n","scores_bertscore_r = []\n","scores_bertscore_f = []\n","scores_f1_token = []\n","scores_cos_sim = []\n","scores_comp_ratio = []\n","scores_sent_split = []\n","scores_lev_sim = []\n","scores_exact = []\n","scores_adds = []\n","scores_dels = []\n","scores_lex_comp = []\n","\n","print(\"\\nBeginning system evaluation:\")\n","for data_file in data_files:\n","    print(\"Evaluating \", data_file)\n","    # sentence_scores = easse.cli.evaluate_system_output(\n","    results = evaluate_system_output(\n","        \"custom\",\n","        sys_sents_path=\"/content/drive/MyDrive/apln552-project/data/raw_data/\" + data_file,\n","        orig_sents_path=\"/content/drive/MyDrive/apln552-project/data/raw_data/classic_lit_complex.txt\",\n","        refs_sents_paths=\"/content/drive/MyDrive/apln552-project/data/raw_data/classic_lit_simplified_human.txt\",\n","        # tokenizer=\"13a\",\n","        # lowercase=True,\n","        metrics=[\n","                'bleu',\n","                'fkgl',\n","                'sari',\n","                'bertscore',\n","                'f1_token',\n","                'cos_sim'\n","                ],\n","        # analysis=False,\n","        quality_estimation=True,\n","        )\n","    scores_bleu.append(results['bleu'])\n","    scores_sari.append(results['sari'])\n","    scores_fkgl.append(results['fkgl'])\n","    scores_bertscore_p.append(results['bertscore_precision'])\n","    scores_bertscore_r.append(results['bertscore_recall'])\n","    scores_bertscore_f.append(results['bertscore_f1'])\n","    scores_f1_token.append(results['f1_token'])\n","    scores_cos_sim.append(results['cos_sim'])\n","    scores_comp_ratio.append(results['quality_estimation']['Compression ratio'])\n","    scores_sent_split.append(results['quality_estimation']['Sentence splits'])\n","    scores_lev_sim.append(results['quality_estimation']['Levenshtein similarity'])\n","    scores_exact.append(results['quality_estimation']['Exact copies'])\n","    scores_adds.append(results['quality_estimation']['Additions proportion'])\n","    scores_dels.append(results['quality_estimation']['Deletions proportion'])\n","    scores_lex_comp.append(results['quality_estimation']['Lexical complexity score'])\n","\n","all_scores = {}\n","all_scores['bleu'] = scores_bleu\n","all_scores['sari'] = scores_sari\n","all_scores['fkgl'] = scores_fkgl\n","all_scores['bertscore_p'] = scores_bertscore_p\n","all_scores['bertscore_r'] = scores_bertscore_r\n","all_scores['bertscore_f'] = scores_bertscore_f\n","all_scores['f1_token'] = scores_f1_token\n","all_scores['cos_sim'] = scores_cos_sim\n","all_scores['comp_ratio'] = scores_comp_ratio\n","all_scores['sent_split'] = scores_sent_split\n","all_scores['lev_sim'] = scores_lev_sim\n","all_scores['exact'] = scores_exact\n","all_scores['adds'] = scores_adds\n","all_scores['dels'] = scores_dels\n","all_scores['lex_comp'] = scores_lex_comp"],"metadata":{"id":"3XS_6k4obHAj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","# Organize data into dataframes (one per metric)\n","all_df = {}\n","for metric in all_scores:\n","    array = np.array(all_scores[metric]).transpose()\n","    all_df[metric] = pd.DataFrame(array, columns=[f for f in data_files])\n","\n","# Add sentences into the dataframes\n","filename = \"/content/drive/MyDrive/apln552-project/data/raw_data/classic_lit_complex.txt\"\n","def read_lines(filename):\n","    with open(filename, encoding=\"utf-8\") as f:\n","        lines = f.readlines()\n","        lines = [x.strip() for x in lines]\n","    return lines\n","sents = read_lines(filename)\n","\n","for metric in all_df:\n","    all_df[metric].insert(0, \"sentence\", sents)\n","\n","# Write to CSV\n","%cd /content/drive/MyDrive/apln552-project/data/eval_results/auto_eval\n","\n","for metric in all_df:\n","    all_df[metric].to_csv(\"sent_level_\"+metric+\".csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQtzwmwoEFvV","executionInfo":{"status":"ok","timestamp":1651799398542,"user_tz":240,"elapsed":194,"user":{"displayName":"Brandon Wilde","userId":"18127879461253566195"}},"outputId":"72de6093-ca5b-4878-9bdb-d31b1ad66562"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1fUU8cfGdJzyPvB9Zonuw3IOAJZqBxivM/apln552-project/data/eval_results/auto_eval\n"]}]}]}